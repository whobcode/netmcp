#!/usr/bin/env python3
"""
Generate an Exploit Database index that can be bundled with the Worker.

Reads the upstream CSV exports (files_exploits.csv / files_shellcodes.csv)
from a local checkout or the official GitLab raw dataset, extracts the most
useful metadata, optionally samples on-disk exploit files for short previews,
and writes a TypeScript module at src/exploitdb-dataset.ts that exports the
resulting structure.

Usage:
    python3 scripts/build_exploitdb.py
    # or via npm:
    npm run sync:exploitdb
"""

from __future__ import annotations

import argparse
import base64
import csv
import io
import json
import os
import sys
import textwrap
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, Iterable, List
from urllib.error import HTTPError, URLError
from urllib.request import Request, urlopen
import gzip

DEFAULT_REMOTE_ROOT = "https://gitlab.com/exploit-database/exploitdb/-/raw/main"
USER_AGENT = "mcp-exploitdb-sync/1.0"


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Build exploitdb dataset module")
    parser.add_argument(
        "--root",
        default=os.environ.get("EXPLOITDB_ROOT", DEFAULT_REMOTE_ROOT),
        help=(
            "Path or URL to the exploit-db checkout "
            "(defaults to the official GitLab raw dataset)"
        ),
    )
    limit_env = os.environ.get("EXPLOITDB_LIMIT")
    limit_default = int(limit_env) if limit_env else None
    parser.add_argument(
        "--limit",
        type=int,
        default=limit_default,
        help=(
            "Maximum number of entries to keep "
            "(omit or set to 0/-1 for the full dataset; default: full dataset)"
        ),
    )
    parser.add_argument(
        "--preview-bytes",
        type=int,
        default=int(os.environ.get("EXPLOITDB_PREVIEW_BYTES", "768")),
        help="Preview length to read from exploit files (default: %(default)s)",
    )
    parser.add_argument(
        "--output",
        default="src/exploitdb-dataset.ts",
        help="Where to write the generated TS module (default: %(default)s)",
    )
    return parser.parse_args()


def is_remote_root(value: str | Path) -> bool:
    return isinstance(value, str) and value.startswith(("http://", "https://"))


def build_source_identifier(root: str | Path, filename: str, remote: bool) -> str | Path:
    if remote:
        base = str(root).rstrip("/")
        return f"{base}/{filename}"
    return Path(root) / filename


def fetch_remote_text(url: str) -> str:
    try:
        request = Request(url, headers={"User-Agent": USER_AGENT})
        with urlopen(request) as response:
            if response.status >= 400:
                raise HTTPError(url, response.status, response.reason, response.headers, None)
            payload = response.read()
    except (HTTPError, URLError) as exc:
        raise RuntimeError(f"Failed to download {url}: {exc}") from exc
    return payload.decode("utf-8", errors="ignore")


def read_csv(path: Path | str, remote: bool) -> List[Dict[str, str]]:
    if remote:
        text = fetch_remote_text(str(path))
        reader = csv.DictReader(io.StringIO(text))
        return [row for row in reader]
    path = Path(path)
    if not path.exists():
        raise FileNotFoundError(f"Missing required file: {path}")
    with path.open(newline="", encoding="utf-8", errors="ignore") as fh:
        reader = csv.DictReader(fh)
        return [row for row in reader]


def safe_split(value: str, delimiter: str = ";") -> List[str]:
    if not value:
        return []
    return [part.strip() for part in value.split(delimiter) if part.strip()]


def read_preview(root: Path | str, file_path: str, preview_bytes: int) -> str | None:
    if is_remote_root(root):
        return None
    target = Path(root) / file_path
    if not target.exists():
        return None
    try:
        with target.open("rb") as fh:
            raw = fh.read(preview_bytes)
        text = raw.decode("utf-8", errors="ignore").replace("\r\n", "\n")
        return text.strip()
    except Exception:
        return None


def coerce_int(value: str) -> int | None:
    try:
        return int(value)
    except (TypeError, ValueError):
        return None


def iso_or_none(value: str) -> str | None:
    if not value:
        return None
    value = value.strip()
    if not value:
        return None
    try:
        # Most fields are already ISO-like. Validate and normalise.
        parsed = datetime.fromisoformat(value)
        return parsed.date().isoformat()
    except ValueError:
        return value


def slice_entries(entries: Iterable[Dict[str, Any]], limit: int | None) -> List[Dict[str, Any]]:
    enriched = []
    for row in entries:
        enriched.append(row)
    enriched.sort(
        key=lambda item: (
            item.get("date_published")
            or item.get("date_added")
            or "1970-01-01",
            item.get("id") or 0,
        ),
        reverse=True,
    )
    if limit and limit > 0 and len(enriched) > limit:
        return enriched[:limit]
    return enriched


def main() -> None:
    args = parse_args()
    root_input = args.root
    remote_root = is_remote_root(root_input)
    root: str | Path
    if remote_root:
        root = root_input
    else:
        root_candidate = Path(root_input)
        if not root_candidate.exists():
            print(
                f"ExploitDB root '{root_candidate}' not found; falling back to remote dataset.",
                file=sys.stderr,
            )
            root = DEFAULT_REMOTE_ROOT
            remote_root = True
        else:
            root = root_candidate

    exploits_source = build_source_identifier(root, "files_exploits.csv", remote_root)
    shellcodes_source = build_source_identifier(root, "files_shellcodes.csv", remote_root)

    print(f"Fetching exploits from {exploits_source}...", file=sys.stderr)
    exploits_csv = read_csv(exploits_source, remote_root)
    print(f"Fetching shellcodes from {shellcodes_source}...", file=sys.stderr)
    shellcodes_csv = read_csv(shellcodes_source, remote_root)

    entries: List[Dict[str, Any]] = []

    def convert(rows: Iterable[Dict[str, str]], kind: str) -> None:
        for row in rows:
            entry = {
                "id": coerce_int(row.get("id") or ""),
                "title": (row.get("description") or "").strip(),
                "file": (row.get("file") or "").strip(),
                "author": (row.get("author") or "").strip() or None,
                "type": (row.get("type") or "").strip() or None,
                "platform": (row.get("platform") or "").strip() or None,
                "codes": safe_split(row.get("codes") or ""),
                "tags": safe_split(row.get("tags") or ""),
                "aliases": safe_split(row.get("aliases") or ""),
                "port": (row.get("port") or "").strip() or None,
                "verified": (row.get("verified") or "").strip() == "1",
                "date_published": iso_or_none(row.get("date_published") or ""),
                "date_added": iso_or_none(row.get("date_added") or ""),
                "date_updated": iso_or_none(row.get("date_updated") or ""),
                "screenshot_url": (row.get("screenshot_url") or "").strip() or None,
                "application_url": (row.get("application_url") or "").strip() or None,
                "source_url": (row.get("source_url") or "").strip() or None,
                "kind": kind,
            }
            preview = (
                read_preview(root, entry["file"], args.preview_bytes)
                if not remote_root
                else None
            )
            if preview:
                entry["preview"] = preview
            entries.append(entry)

    convert(exploits_csv, "exploit")
    convert(shellcodes_csv, "shellcode")

    selected = slice_entries(entries, args.limit)

    dataset = {
        "generated_at": datetime.utcnow().replace(microsecond=0).isoformat() + "Z",
        "source_root": str(root),
        "entry_count": len(selected),
        "total_available": len(entries),
        "limit": args.limit,
        "entries": selected,
    }

    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    json_payload = json.dumps(dataset, ensure_ascii=False, separators=(",", ":"))
    compressed = gzip.compress(json_payload.encode("utf-8"), compresslevel=9)
    encoded = base64.b64encode(compressed).decode("ascii")
    wrapped = "\n".join(textwrap.wrap(encoded, width=120))

    # Generate TypeScript module (uses DecompressionStream instead of Node.js zlib)
    module_source = (
        "// @generated by scripts/build_exploitdb.py â€” do not edit by hand\n"
        "// Run `npm run sync:exploitdb` to refresh this dataset.\n"
        f"// Generated: {dataset['generated_at']}\n"
        f"// Entries: {dataset['entry_count']} (from {dataset['total_available']} total)\n\n"
        "export const EXPLOITDB_DATASET_BASE64 = `\n"
        f"{wrapped}\n"
        "`;\n"
    )

    output_path.write_text(module_source, encoding="utf-8")

    print(
        f"Wrote {output_path} with {dataset['entry_count']} entries "
        f"(from {dataset['total_available']} total). "
        f"Compressed payload: {len(compressed)} bytes (base64 {len(encoded)} chars)."
    )


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        sys.exit(1)
